{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewo_ccxAp2nS",
        "colab_type": "text"
      },
      "source": [
        "## Face Recognition\n",
        "**Methods**: Haar's cascades, Local Binary Patterns.\n",
        "\n",
        "**Instruments**: Python, OpenCV (3.4.3), PIL(4.3.0), NumPy (1.16.4)\n",
        "\n",
        "** Interesting notes **:\n",
        "1. OpenCV doesn't support gif format, so that's why we use Image module (from PIL)to read images in grayscale format and convert them into numpy arrays. \n",
        "If you have images in another format - do not use this module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlUlEXFDpugR",
        "colab_type": "code",
        "outputId": "d1dc59df-0666-4671-bac0-1939a9cfc029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "print(\"cv2 version: \" + cv2.__version__)\n",
        "print(\"numpy version: \" + np.__version__)\n",
        "print(\"PIL version: \" + PIL.__version__)\n",
        "\n",
        "# Haar's cascades\n",
        "cascadePath = \"/mnt/haarcascade.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascadePath)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cv2 version: 3.4.3\n",
            "numpy version: 1.16.4\n",
            "PIL version: 4.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hi45SY-cMSH",
        "colab_type": "text"
      },
      "source": [
        "### Theory\n",
        "Object Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n",
        "\n",
        "Here we will work with face detection. Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. For this, Haar features shown in the below image are used. They are just like our convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYWs9AwDcZnr",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://docs.opencv.org/3.4/haar.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24PW44Wcb8S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now, all possible sizes and locations of each kernel are used to calculate lots of features. (Just imagine how much computation it needs? Even a 24x24 window results over 160000 features). For each feature calculation, we need to find the sum of the pixels under white and black rectangles. To solve this, they introduced the integral image. However large your image, it reduces the calculations for a given pixel to an operation involving just four pixels. Nice, isn't it? It makes things super-fast.\n",
        "\n",
        "But among all these features we calculated, most of them are irrelevant. For example, consider the image below. The top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applied to cheeks or any other place is irrelevant. So how do we select the best features out of 160000+ features? It is achieved by Adaboost.\n",
        "\n",
        "For this, we apply each and every feature on all the training images. For each feature, it finds the best threshold which will classify the faces to positive and negative. Obviously, there will be errors or misclassifications. We select the features with minimum error rate, which means they are the features that most accurately classify the face and non-face images. (The process is not as simple as this. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then the same process is done. New error rates are calculated. Also new weights. The process is continued until the required accuracy or error rate is achieved or the required number of features are found).\n",
        "\n",
        "The final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can't classify the image, but together with others forms a strong classifier. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features. (Imagine a reduction from 160000+ features to 6000 features. That is a big gain).\n",
        "\n",
        "So now you take an image. Take each 24x24 window. Apply 6000 features to it. Check if it is face or not. Wow.. Isn't it a little inefficient and time consuming? Yes, it is. The authors have a good solution for that.\n",
        "\n",
        "In an image, most of the image is non-face region. So it is a better idea to have a simple method to check if a window is not a face region. If it is not, discard it in a single shot, and don't process it again. Instead, focus on regions where there can be a face. This way, we spend more time checking possible face regions.\n",
        "\n",
        "For this they introduced the concept of Cascade of Classifiers. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one. (Normally the first few stages will contain very many fewer features). If a window fails the first stage, discard it. We don't consider the remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region. How is that plan!\n",
        "\n",
        "The authors' detector had 6000+ features with 38 stages with 1, 10, 25, 25 and 50 features in the first five stages. (The two features in the above image are actually obtained as the best two features from Adaboost). According to the authors, on average 10 features out of 6000+ are evaluated per sub-window.\n",
        "\n",
        "So this is a simple intuitive explanation of how Viola-Jones face detection works. Read the paper for more details or check out the references in the Additional Resources section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW8c3MxncQs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "\n",
        "def get_images(path):\n",
        "  image_paths = [os.path.join(path, f) for f in os.listdir(path) if not f.endswith('.happy')]\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    image_gray = Image.open(image_path).convert('L') #Adds or replaces the alpha layer in this image. If the image does not have an alpha layer, it’s converted to “LA” or “RGBA”. The new layer must be either “L” or “1”.\n",
        "    image = np.array(image_gray, 'uint8') #convert the image format into np.array\n",
        "\n",
        "    #get the label of the images\n",
        "    label = int(os.path.split(image_path)[1].split(\".\")[0].replace(\"subject\", \"\"))\n",
        "    faces = faceCascade.detectMultiScale(image)\n",
        "\n",
        "    #if face is detected, append the face to images and the label to labels\n",
        "    #We are appending all the absolute path names of the database images in the list images_path. \n",
        "    #We, aren't appending images with the .sad extension, as we will use them to test the accuracy of the recognizer\n",
        "    for (x, y, w, h) in faces:\n",
        "      images.append(image[y: y + h, x: x + w])\n",
        "      labels.append(label)\n",
        "      cv2.imshow(\"Add faces to training set\", image[y: y + h, x: x + w])\n",
        "      cv2.waitKey(50)\n",
        "  return images, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJCA6IRdlSRh",
        "colab_type": "text"
      },
      "source": [
        "**cv2.waitKey** we loop around each images to detect the face in it and update our 2 lists\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksuoJAKvg77J",
        "colab_type": "code",
        "outputId": "6bc37651-4c3c-48b2-97bb-4a66104c215e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "path = '/tmp'\n",
        "\n",
        "#get faces and their labels\n",
        "images, labels = get_images(path)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "#train the model\n",
        "recognizer.train(images, np.array(labels))\n",
        "\n",
        "# Append the images with the extension .sad into image_paths\n",
        "image_paths = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.sad')]\n",
        "\n",
        "for image_path in image_paths:\n",
        "predict_image_pil = Image.open(image_path).convert('L')\n",
        "predict_image = np.array(predict_image_pil, 'uint8')\n",
        "faces = faceCascade.detectMultiScale(predict_image)\n",
        "\n",
        "for (x, y, w, h) in faces:\n",
        "    label_predicted, conf = recognizer.predict(predict_image[y: y + h, x: x + w])\n",
        "    label_actual = int(os.path.split(image_path)[1].split(\".\")[0].replace(\"subject\", \"\"))\n",
        "    \n",
        "    if nbr_actual == label_predicted:\n",
        "        print \"{} is Correctly Recognized with confidence {}\".format(label_actual, conf)\n",
        "    else:\n",
        "        print \"{} is Incorrectly Recognized as {}\".format(nbr_actual, label_predicted)\n",
        "    cv2.imshow(\"Recognizing Face\", predict_image[y: y + h, x: x + w])\n",
        "    cv2.waitKey(1000)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e2e7c4cf7e51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#get faces and their labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-df350b59672f>\u001b[0m in \u001b[0;36mget_images\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Add faces to training set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_import_hooks/_cv2.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ]
        }
      ]
    }
  ]
}